\chapter{Input Technologies and Human Computer Interaction}
\label{ch:input}

An important goal for any user interface is to have the human-computer interaction be as intuitive and natural as possible.
Natural interaction is critical for a sketching interface; while younger, more technologically willing professionals flock to digitized content creation software, those classically trained prefer to stick to physical methods.
In order to convince the latter to adapt to a new system, the transition process must be as smooth as possible, so as many of the techniques utilized in the physical system should be emulated as closely as technology allows in the digital system.

A key element for bridging the gap between physical and digital tools are input devices, computer hardware used to control electronic devices.
For a long time, the most common input devices for digital environments have been the mouse and keyboard, which provide rather unintuitive ways to move and control the computer.
However, in recent years there have been a number of advancements in the human-computer interaction field, which has produced a wide array of input devices.
In this chapter, we will discuss human-computer interaction principles used to decide how to best utilize input devices, as well as discuss the particular devices needed to create a good sketching system.



\section{Input Classifications}

\subsection{Modality}

In human computer interaction, a modality is a channel of sensory input/output between the human and the computer. Modalities can be split into two general types: human-computer, and computer-human modalities.

Human-computer modalities describe the ways humans input information to the computer.
These modalities are devices and sensors attached to the computer.
Common input device modalities are keyboards, pointing devices, and touch screens, while more complex modalities are computer vision, speech recognition, gesture recognition, and orientation.
In sketching, the equivalent of a human-computer modality is the sketch implement. 
Whether the tool is a pen, pencil, charcoal, or something else, the artist provides "data" to the sketching surface through his strokes by sending information like position, angle, and pressure.

Computer-human modalities describe the ways the computer outputs information to communicate with humans.
This requires stimulation of one of the human senses: sight, hearing, taste, smell, touch, balance, temperature, and pain. 
Of these, modern input devices generally communicate using sight and hearing, because they are capable of sending information at much higher speeds and larger bandwidths, as well as being the more common ways humans communicate with each other.
There are also small uses of haptics, general vibrations or other movement, to provide feedback, but generally they are accompanied by visual and auditory cues.

While working on this project, it became apparent that professionals are very sensitive to a complex combination of input and output modalities, some of which are not reflected well by current hardware capabilities. 
For example, an architect commented while drawing on the large display that he felt uncomfortable because the digital pen pressing against the display didn't feel like a physical pencil brushing against a piece of paper. 
The difference in friction and materials meant that he was unsure of how things like pressure and angles effected his drawing.
This problem is a result of both the limitations of the input device, as well not providing proper feedback for his actions.
On this project, we focus more on the input modalities, but for a truly realistic sketching system, specialized hardware must be utilized in order to closely emulate these intricacies.

\subsection{Direct and Indirect}
Direct and indirect input refer to how the input space corresponds to the display space. 
With direct input, the two are directly correlated. 
For example, on a touch screen, where you touch is where the input is recorded. 
With indirect input devices, the input value is only relative to the input space. Moving a mouse three inches can correspond to movement of any kind on the computer screen. 
For example, in desktop settings, moving the mouse to the left three inches can correspond to a variety of moments on the computer screen. 
In a three-dimensional application it can cause the user to move left, or turn left.

Indirect input devices have the advantage of being more application independent. However, this comes at the cost of accessibility. 
A mouse can do many things, but the user must learn what it does in the current context. 
Direct input methods tend to be intuitive while limited in scope. 
The user knows touching a screen at a spot causes an action at that spot.

As will be described later, when physically sketching, most if not all interactions are direct. 
There are minute indirect details such as different drawing implements which can produce different strokes with the same input, but on a high level there is a one-to-one spatial correlation between actions and results.
Therefore, for this project, it is important to minimize any indirect interaction between the user and the system.
For any indirect interaction, we should try to minimize the complexity of the difference between the input and output spaces.

\section{Input Devices}
\subsection{Keyboard}

A keyboard is an indirect input device using an arrangement of buttons or keys, acting as mechanical levels or electronic switches. 
Common keyboards are typewriter style devices, with many buttons representing alphanumeric characters as well as a small number of additional function keys.
Desktop keyboards usually have from 100 to 105 keys, while laptop and other small device keyboards contain less.
All standard keyboards have a typing area used for letters of the alphabet, numbers, punctuation, and other basic characters.
However, there are also composite devices that have keyboard like features, such as video game controllers. 
Game controller buttons offer contextual and situational functionality depending on the application in use.


There are certainly auxiliary areas in good sketching software where a keyboard of some kind should see use; for example, saving files, but for the core of the application, the user should never use one. 
Many design applications such as Maya use a combination of function and character keys to perform actions, bringing a significant learning curve in order to use these applications effectively.
This results in an interface that is unnatural, and requires application specific skill-sets.
For this project, we try to avoid use of the keyboard as much as possible.


\subsection{Pointing Devices}
 A pointing device is a device that more easily allows a user to input spacial data to a computer. 
 Many common input devices fall under this classification. 
 CAD systems and graphical user interfaces allow the user to control and provide data to the computer using physical motion.
 These movements are then echoed on the screen in some way, whether it be by an on-screen cursor, or some change in the visual output.
 Pointing devices are usually controlled by either physical movement of an object, or touching a surface.
 Examples of devices based on motion are the mouse, trackball, and joystick, and those based on touch are the graphics tablet, stylus, touchpad, and touch screen.
 
 \subsubsection{Mouse}
 A mouse is a small, hand-held device that is pushed over a flat, horizontal surface.
 Older mice used a physical ball at the base of the device, combined with sensors to detect when the ball rotates.
 When the mouse moves and rotation is sensed, the distance and directional information is sent from the mouse to the computer.
 A more modern approach is the optical mouse, which uses infrared light instead of a roller to detect changes in position.
 
 The mouse is the oldest pointing device, and used with every desktop computer.
 The mouse works in an indirect space as the position of the mouse and that of the cursor on screen are completely unrelated.
 When a mouse is moved ten inches to the left, the movement of the mouse cursor on screen is not ten inches, but some function with ten inches as input.
 As discussed previously, we would like to eliminate indirect input where ever possible.
 While it will be possible to use the mouse with our application, it should be seen as legacy functionality.

 \subsubsection{Tablet}
 A graphics or digitizing tablet is a special tablet that is similar to a touchpad. 
 However, it is controlled with a digital pen or stylus that is held and used like a normal pen.
 An alternative control device for tablets is called a puck. 
 This is a mouse-like device that can detect absolute position and rotation.
 Professional pucks used for detailed CAD work have a reticle which allows the user to see the exact point on the tablet's surface targeted by the puck.
 Graphics tablets are commonly used to create 2D computer graphics because of their input similarities to traditional drawing techniques.
 Tablets are very commonly used for digital sketching applications.
 However, many graphics tablets are used in combination with a screen, meaning there is still a visual and physical disconnect between the sketch and the output.
 
 \subsubsection{Pen}
 
 As mentioned above, touch panels can be used with rigid styli to simulate the experience of writing. 
 However, the pen itself can also be computerized to capture additional information for various applications. 
 An active pen is an input device that includes electronic components and allows users to write directly onto the display surface of a computing device.
 The active pen's electronic components generate wireless signals that are picked up by a built-in digitizer and transmitted to its dedicated controller, providing data on pen location, pressure and other functionalities. 
 Additional features enabled by the active pen's electronics include \textit{palm rejection} to prevent unintended touch inputs, and \textit{hover}, which allows the computer to track the pen's location when it is held near, but not touching the screen. 
 Most active pens feature one or more function buttons (e.g. eraser and right-click) that can be used in place of a mouse or keyboard.

\subsection{Touch Screen Devices}

A touch screen can be considered as an input device layered on top of a display. 
Users provide input by touching the screen with one or more fingers, or with a special stylus/pen.
This method of input allows users to directly interact with what is displayed, as opposed to using an indirect input device such as a mouse or touchpad.

One major advantage touch screens offer over other input devices is ease of use.
While frequent computer users are familiar with using a mouse and keyboard, touching icons on a screen is intuitive even for those with limited to no computer experience.
This ease of use can reduce the learning curve and increase productivity when using these types of user interfaces.
Touchscreens are also faster to use than traditional input methods.
When a user interacts with a computer using a mouse and keyboard, there are many small adjustments the user needs to make; they need to locate the pointer, and adjust for the mouse acceleration.
Direct interaction allows for users to interact with the computer without worrying about correlating the interaction space to the virtual space. 

There are many ways to build a touch screen.
The key points in any implementation are to recognize one or more input points touching a display, to interpret the command these touches represent, and to communicate with an application.
The three main types of touch sensing technologies are resistive sensing, surface acoustic wave sensing, and capacitive sensing.

\subsubsection{Resistive Sensing}
\begin{figure}
	\centering
		\fbox{\includegraphics[width=0.98\textwidth]{resistive}}
	\caption[A diagram of a resistive touch sensor] {When a user interacts with a resistive touch panel, they press against a protective polyester film. At the touch point, two sheets of electrode covered film press against each other, causing an electric current. \cite{touchtech}} 
	\label{fig:resistivetouch}
\end{figure}
Resistive touch panels are composed of two thin, flexible sheets coated with a resistive material and separated by a small gap.
A resistive touch monitor features a simple internal structure: a resistive panel is placed on top of a glass screen, with a polyester film screen on top of the panel used as the contact surface. 
Pressing the surface of the film screen causes the electrode-covered sheets between the film and glass panels to come into contact, resulting in the flow of electrical current. 
The point of contact is identified by detecting this change in voltage. (Figure~\ref{fig:resistivetouch})

Resistive technology is low cost due to the simple structure of the touch screen and controller circuit.
Analog sensors have high resolution, the most common being 4096 by 4096 dots-per-inch (DPI), as well as high accuracy, while consuming low amounts of power during operation.
This DPI refers to the number of sensing dots-per-inch, and is uncorrelated to the display DPI.
Resistive touch screens can be used with any object that can provide a pressure point, since the system only requires contact. 
This allows for users to use pens and gloves to interact with a device.
Disadvantages include it's poor responsiveness compared to other developed sensing methods, generally requiring harder presses, providing lower light transmittance causing a reduction in screen quality, and a decrease in accuracy with large screen sizes (\textgreater 24 inches).
While high resolutions would allow for more accurate and more detailed sketching, the poor responsiveness would make resistive panels a poor substitute for traditional drawing media.

\begin{center}
\begin{figure}
\fbox{\includegraphics[width=0.98\textwidth]{saw}}
\caption[A diagram of a surface acoustic wave sensor] {A surface acoustic wave sensor transmits vibrations across the surface of a glass screen. When the screen is touched, the vibrations are absorbed and attenuated by the touch object. \cite{touchtech}}
\label{fig:sawsensing}
\end{figure}
\end{center}
 
\subsubsection{Surface Acoustic Wave (SAW) Sensing}

Surface acoustic wave (SAW) technology was developed to achieve bright touch panels displays with high levels of visibility; mainly to address the drawbacks of low light transmittance in resistive film touch panels. 
These are also called surface wave or acoustic wave touch panels. 
Aside from standalone LCD monitors, these are widely used in public spaces in devices like point-of-sale terminals, ATMs, and electronic kiosks.

These panels detect the screen position where contact occurs with a finger or other object using the attenuation in ultrasound elastic waves on the surface. 
The internal structure of these panels is designed so that multiple piezoelectric transducers arranged in the corners of a glass substrate transmit ultrasound surface elastic waves as vibrations in the panel surface, which are received by transducers installed opposite the transmitting ones. 
When the screen is touched, ultrasound waves are absorbed and attenuated by the finger or other object. 
The location is identified by detecting these changes.
(Figure~\ref{fig:sawsensing})



The strengths of this type of touch panel include high light transmittance and superior visibility, since the structure requires no film or transparent electrodes on the screen.
Structurally, this type of panel ensures high stability and long service life, free of changes over time or deviations in position. 
Even if the surface does somehow become scratched, the panel remains sensitive to touch.  
Weak points include compatibility with only fingers and soft objects (such as gloves) that absorb ultrasound surface elastic waves. 
These panels require special-purpose styluses and may react to substances like water drops or small insects on the panel.
This sensing method, like resistive sensing, is more suitable for public displays that see heavy use.
While high quality displays are desirable, the relative inflexibility of input capabilities limit SAW powered displays for use in sketching applications.



\subsubsection{Capacitive Sensing}
Capacitive touch panels use the natural flow of electricity through the human body, also called body capacitance, as the input signal.
They are most commonly used in consumer level hardware such smart phones, tablets, and LCD monitors.
They are constructed from a wide variety of materials, such as copper, Indium tin oxide (ITO), and printed ink.
Unlike resistive film touch panels, capacitive touch panels do not respond to touch by clothing or standard styli.
They feature strong resistance to dust and water drops and high durability and scratch resistance. 
In addition, their light transmittance is higher, as compared to resistive film touch panels allowing for higher quality displays.
There are two types of capacitive technology; surface capacitive and projected capacitive systems.
 
\begin{center}
\begin{figure}
\fbox{\includegraphics[width=0.98\textwidth]{surcap}}
\caption[A diagram of a surface capacitive sensor]{In a surface capacitive sensor, a uniform electric field is generated over a glass panel from electrodes on the panel's four corners. When a user touches the panel, current flows from each of the corners through the finger.  \cite{touchtech}}
\label{fig:surcap}
\end{figure}
\end{center}

\paragraph{Surface Capacitive Sensing}

is often used for larger sized displays (over 14 inches) that are used by the general public because of their high durability and high screen quality.
Surface capacitive displays can be seen on ATM machines, ticket kiosks, arcade games, automation devices in factories and offices, and in the medical industry.

A surface capacitive panel is constructed using a glass sheet. 
A transparent conductive coating is placed over the sheet, and a glass protective coating is placed above that. 
Electrodes are placed on the four corners of the panel. 

If the same phase voltage is imposed to the electrodes on the four corners, then a uniform electric field will be formed over the panel. 
When a finger touches the panel, electrical current flows from each of the four corners through the finger. 
The ratio of the electrical currents flowing from each of the four corners is measured to detect the touched point. 
The measured current values will be inversely proportional to the distance between the touched point and each of the four corners. (Figure~\ref{fig:surcap})

A surface capacitive touch panel has a simpler structure than a projected capacitive touch panel (see below).
This allows for lower cost in production, high durability, and high visibility due to the main structure being a single glass layer. 
However, it's simplistic structure also means it is structurally difficult to detect two or more contact points simultaneously.
Surface capacitive can usually only detect bare finger touches, although some may detect touches through a thin pair of gloves.
Surface scratches can cause touch signals to get interrupted.
Some surface capacitive displays support pen writing, but not simultaneous pen and touch.

\paragraph{Projected Capacitive Sensing}

is often used for smaller screen sizes than surface capacitive touch panels. 
They've attracted significant attention in mobile devices. 
The iPhone, iPod Touch, and iPad all use this method to achieve high-precision multi-touch functionality and high response speed.
However, the technology is also now being used for large display devices such as Microsoft's Surface Hub, a 55-inch projected capacitive display.

\begin{figure}
\fbox{\includegraphics[width=0.98\textwidth]{projcap}}
\caption[A diagram a projected capacitive sensor]{In a projected capacitive sensor, the display is covered by a layer of numerous transparent electrodes positioned in specific patterns. When a finger approaches the surface, electrostatic capacity among multiple electrodes changes simultaneously. \cite{touchtech}}
\label{fig:projcap}
\end{figure}

The internal structure of these touch panels consists of a substrate incorporating an IC chip for processing computations, over which is a layer of numerous transparent electrodes positioned in specific patterns. 
The surface is covered with an insulating glass or plastic cover. 
When a finger approaches the surface, electrostatic capacity among multiple electrodes changes simultaneously, and the position where contact occurs can be identified precisely by measuring the ratios between these electrical currents. (Figure~\ref{fig:projcap})

A unique characteristic of a projected capacitive touch panel is the fact that the large number of electrodes enables accurate detection of contact at multiple points (multi-touch).
Smaller projected capacitive multi-touch panels, such as those found in smart phones and tablets, are made with indium-tin-oxide.
However, the methods used to make small panels are poorly suited for use in large screens, since increased screen size results in a slower transmission of electrical currents across the panel, increasing the amount of error and noise in detecting the points touched.
Instead, larger touch panels use center-wire projected capacitive touch panels in which very thin electrical wires are laid out in a grid as a transparent electrode layer.
While lower resistance makes center-wire projected capacitive touch panels highly sensitive, they are less suited to mass production than ITO etching.

\subsection{Summary of Input Technologies}

In Table~\ref{tab:touchtech}, we provide an overview of the touch technologies discussed in the previous sections.
For a pure 2-D sketching interface, comparing the capabilities indicates that a resistive panel would likely be the best input device.
It's high accuracy and resolution are very desirable traits for creating high quality, accurate sketches.
However, our system intends to use multi-touch gestures to navigate the three-dimensional sketching environment.
Therefore, we require multi-touch capabilities that a resistive panel lacks.
As a result, we design our application around capacitive displays, as they have the best touch functionality.
Their poor stylus support can be augmented by the use of active pen technology specially designed for capacitive displays.
In particular, we use Microsoft's Surface Hub technology, which has a basic pen capability, upwards of 4K resolution, and support for up to a hundred touch points.

\begin{table}
\label{tab:touchtech}
\begin{center}
\begin{tabular}{| p{5cm} | l | p{3cm} | l |}
\hline
\textbf{Method} & \textbf{Resistive} & \textbf{Capacitive} & \textbf{SAW} \\ \hline
\textbf{Light Transmittance} & Poor & Good & Good \\ \hline
\textbf{Finger Touch} & Excellent & Excellent & Excellent \\ \hline
\textbf{Gloved Touch} & Excellent & None & Good \\ \hline
\textbf{Stylus Touch} & Excellent & Poor & Good \\ \hline
\textbf{Maximum Single User Touch Points} & One & Ten & Two \\ \hline
\textbf{Accuracy} & Excellent & Good & Good \\ \hline
\textbf{Durability} & Poor & Excellent & Excellent \\ \hline
\textbf{Water Resistance} & Excellent & Excellent & Poor \\ \hline
\textbf{Cost} & Reasonable & Not reasonable & Not reasonable \\ \hline
\end{tabular}
\end{center}
\caption[A comparison of touch technology]{A comparison of touch technology.}
\end{table}

\section{Advanced User Interaction: Input using Gesture}

A gesture is a form of communication where visible body action communicate particular messages.
Common gestures are usually performed by hand and arm movements.
Other forms of physical non-verbal communication, such as purely expressive display, proxemics, and joint attention differ from gestures, which communicate specific messages.
While some gestures are ubiquitous, such as pointing, which differs little in intent from one application to another, many do not have universal meanings and are defined differently in different disciplines.

\subsection{Types of Gestures}

In \textit{Gestures} \cite{morris79}, Morris describes two main types of gestures: primary and incidental.

Primary gestures are voluntary movements that a person uses with intent of communicating a message. There are three main types:
\begin{enumerate}
\item Emblems: These are gestures that have a direct verbal equivalent. 
For example, a waving of a hand upon an encounter means hello. 
Emblems tend to form in situations where speech is challenging or impossible.
For example, airport controllers on runways communicate with gestures because the planes make it impossible to hear.
\item Illustrators: These gestures are closely linked with speech, and serve to clarify, or add to the content of the message. 
Illustrators are made by hand movements.
A common example of an illustrator is pointing.
\item Reinforcers: There are gestures that help regulate the flow of conversation.
For example, a head nod during conversation can mean that the current speaker should continue, or an upwards point might mean to wait to continue speaking.
\end{enumerate}

Secondary, or incidental gestures are unintentional, but despite their lack of a direct message, are still important in conversation.
Gestures such as grooming the hair, fidgeting, looking down or away, or looking at a clock are all examples of involuntary gestures.
While they do not directly communicate, secondary gestures can still send information about the current state of their user.
This is called leakage, when true feelings or attitudes are revealed despite what the overt signals are communicating.
For example, a man in a rush might say, "Yeah, I can talk" while looking at a watch.
Secondary gestures are not important for Human Computer Interaction, however care must be taken so involuntary gestures are not accidentally used as input. 

Human computer interaction further distinguishes between types of gestures, splitting them into two major categories.
\begin{enumerate}
\item Offline: These gestures are processed after the user interaction with the object. For example, drawing a circle activates a menu.
\item Online: These gestures directly manipulate an object. A common example is taking two fingers and spreading them while touching an object to zoom in on the object.
\end{enumerate} 



\subsection{Multi Touch Gestures}
\begin{figure}
\fbox{\includegraphics[width=\textwidth]{CoreGestures}}
\caption[The core set of gestures]{A visual representation of the core set of touch gestures. \cite{touchreference} } 
\label{fig:touchgestures}
\end{figure}

Multi-touch gestures are predefined motions used to interact with multi-touch devices.
Many modern consumer electronics like smart-phones, tablets, laptops, or desktop computers feature functions triggered by multi-touch gestures.
They tend to be direct input methods with simple to understand functionality heavily based on the gesture used, allowing non-technical people to quickly configure and navigate multi-touch applications.
This section will only discuss core multi-touch gestures, and not the many subsets of unique gestures that combinations of this set of core gestures can create. Visual representations of how these gestures are performed can be seen in Figure~\ref{fig:touchgestures}

\subsubsection{Tap}

Taps are performed by quickly pressing and releasing a screen with a fingertip. 
Most systems differentiate between single and double taps, performed by tapping the screen once or twice respectively.
Taps are generally used for selecting items, with single and double taps offering different types of selection.
Taps are both online and offline gestures, since the number of taps need to be processed, but directly interact with objects being tapped.

\subsubsection{Drag}

Drags are performed by moving a fingertip over a surface without losing contact. 
An alternative type of drag is the flick, which is performed by the same method as the drag, only faster.
Drags are generally used to move elements, making them online gestures.

\subsubsection{Pinch and Spread}

Pinches are performed by taking any number of fingertips and enclosing them towards a point. Spread gestures are the inverse action. Generally these online gestures are used for magnification.

\subsubsection{Rotate}

Rotate gestures are performed by moving two or more fingertips in a circular pattern around a point. The rotation point is used as the input location.
Rotation is an online gesture.

\subsubsection{Press}

Presses are performed by touching a screen for an extended period of time. 
Presses can be combined with other touch gestures to allow for a deeper level of user interaction.
Presses are offline gestures, since the duration of the press must be processed.
This action usually results in context menus which are used for enhancing information about the object.



\subsection{Pen Gestures}

\begin{figure}
\includegraphics[width=\textwidth]{pen-gestures}
\caption[Examples of pen gestures]{Examples of pen gestures and their associated functions. Note that many gestures have shapes unrelated to their functionality.
\cite{inkgestures}}
\label{fig:pengestures}
\end{figure}

Pen gestures recognize certain shapes, not as handwriting, but as an indicator of a special command.
For example, a pig-tail shape (used often as a proofreader's mark) would indicate a delete operation. 
Depending on the implementation, what is deleted might be the object or text where the mark was made, or the stylus can be used as a pointing device to select what it is that should be deleted.
These types of gestures are offline.
Pen gestures tend to be abstract, with functionality not necessarily representative of the shape formed by the gesture. (Figure~\ref{fig:pengestures})

Pens can be used to perform many of the multi-touch gestures, but the pen is just treated as a finger in these scenarios.
In order for our project to incorporate pen gestures, the pen input would need to be preprocessed before it is projected into 3-D space.
While pen gestures are a useful tool, in a free-form sketching environment it can be unclear if the user intends to make a stroke or a gesture, even if using the same symbol.
This can potentially confuse the user.
We can use touch gestures to accomplish similar results, and thus did not incorporate pen gestures in our application.




\subsection{Three Dimensional Gesture Recognition}

While older forms of gesture recognition attempt to translate physical interaction with an input device in order to form gesture based commands, more modern approaches directly interpret motions of the human body.
This is accomplished using computer vision techniques, as well as different types of cameras and sensors used to capture and understand a three dimensional environment.
Once this information is captured a variety of techniques can be used to analyze the scene and detect gestural information. 
While 3-D gesture recognition is an emerging area, and is beginning to see common use in a number of modern user interfaces, it's use for 3-D sketching is left to future work.
Possible applications are using 3-D gestures to manipulate the environment in conjunction with 2-D gestures.

\section{Summary}
In this chapter we discussed how users interact with digital technology, an overview of common and project related input devices, and why certain interaction methods are preferable for 3-D sketching.
For our application, we chose to support a subset of these techniques and devices in order to mimic real world sketching as closely as possible.
We use a capacitive touch display, which supports a very large number of touch input points as well as an active pen device.
From the active pen, we leverage the capability to detect sub-pixel input position, as well as the ability to detect how hard the user presses on the screen with the pen.